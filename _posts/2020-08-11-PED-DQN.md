---
layout:     post
title:      Inducing Cooperation through Reward Reshaping based on Peer Evaluations in Deep Multi-Agent Reinforcement Learning
subtitle:   Reading notes
date:       2020-08-11
author:     Haoxin
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - Multi-Agent Reinforcement Learning
    - Deep Reinforcement Learning
    - Cooperation
---

## Abstract  
This paper proposed a **deep reinforcement learning** algorithm for **semi-cooperative** multi-agent tasks.  
*Here, the word "semi-cooperative" means agents are equipped with their separate reward functions, yet with some willingness to cooperate.*  
The algorithm in the paper is called Peer Evaluation-based Dual DQN (PED-DQN), it proposes to give *peer evaluation signals* to observed agents.  

```
Here, "peer evaluation signals" quantify how agents strategically value a certain transition and this could be regarded as agents' 'selfish'.
```



## Introduction  
The translation from single-agent to multi-agent settings is difficult, the authors explain this from this aspect:  
> *"These simultaneous policy updates make the environment’s transition probabilities non-stationary, which in turn makes learning difficult for the agents because a slight change in one’s policy may cause another agent’s policy to perform sub-optimally."*  

### Brief explaination to fully cooperative cases  
Agents agree to cooperate unconditionally, e.g. being controlled by a single authority. In these cases, a set of agents usually learn to maximize a global  reward and thus the reward function itself can naturally induce a certain level of cooperation.  
*Here, 'Global reward' means that each agent receive the same (scalar) reward value for each
step.*  
