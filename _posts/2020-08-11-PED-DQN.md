---
layout:     post
title:      Inducing Cooperation through Reward Reshaping based on Peer Evaluations in Deep Multi-Agent Reinforcement Learning
subtitle:   Reading notes of one paper in AAMAS 2020
date:       2020-08-11
author:     Haoxin
header-img: img/post-bg-desk.jpeg
catalog: true
tags:
    - Multi-Agent Reinforcement Learning
    - Deep Reinforcement Learning
    - Cooperation
    - AAMAS
---

## Abstract  
This paper proposed a **deep reinforcement learning** algorithm for **semi-cooperative** multi-agent tasks.  
*Here, the word "semi-cooperative" means agents are equipped with their separate reward functions, yet with some willingness to cooperate.*  

The algorithm in the paper is called Peer Evaluation-based Dual DQN (PED-DQN), it proposes to give *peer evaluation signals* to observed agents.  
*Here, "peer evaluation signals" quantify how agents strategically value a certain transition and this could be regarded as agents' 'selfish'.*  


## Introduction  
The translation from single-agent to multi-agent settings is difficult, the authors explain this from this aspect:  
> *"These simultaneous policy updates make the environmentâ€™s transition probabilities non-stationary, which in turn makes learning difficult for the agents because a slight change in oneâ€™s policy may cause another agentâ€™s policy to perform sub-optimally."*  

#### Fully cooperative cases  
Agents agree to cooperate unconditionally, e.g. being controlled by a single authority. In these cases, a set of agents usually learn to maximize a global  reward and thus the reward function itself can naturally induce a certain level of cooperation.  
*Here, 'Global reward' means that each agent receive the same (scalar) reward value for each step.*  
æ‰€ä»¥ï¼Œå®Œå…¨åˆä½œçš„æƒ…å†µæ˜¯æŒ‡æ‰€æœ‰agentéƒ½æ— æ¡ä»¶åˆä½œï¼Œå—ä¸­å¤®åŒä¸€æŽ§åˆ¶ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç»Ÿä¸€çš„ä¸€ä¸ªglobal rewardã€‚  
#### Semi-cooperative
åœ¨å…¶ä»–çš„æƒ…å†µä¸­ï¼Œagentä¼šè€ƒè™‘è‡ªèº«çš„åˆ©ç›Š(interests)ï¼Œä½†æ˜¯ä¹Ÿèƒ½ä»Žåˆä½œä¸­èŽ·åˆ©ï¼Œè¿™ç§æƒ…å†µå°±ç§°ä¸ºâ€œsemi-cooperativeâ€œï¼š  
> Agents may each have its own separate reward function, but is willing to cooperate if an incentive for cooperation is appropriately provided.  
> Semi-cooperative tasks are those with separate per-agent reward functions but whose agents may nonetheless benefit from cooperation.  

The key challenge in semi-cooperative tasks is: we use separate reward functions to represent the selfishness of each agent, yet this often results in agent's choice of non-cooperative actions.  
è¿™å¥è¯çš„æ„æ€æ˜¯ï¼Œæˆ‘ä»¬é€šå¸¸ç”¨ç‹¬ç«‹çš„reward functionæ¥è¡¨ç¤ºæ¯ä¸ªagentçš„selfishnessï¼Œä½†æ˜¯è¿™é€šå¸¸ä¼šå¯¼è‡´agenté€‰æ‹©éžåˆä½œçš„åŠ¨ä½œï¼Œè¿™ä¹Ÿæ˜¯åŠåˆä½œæƒ…å†µä¸­çš„å…³é”®é—®é¢˜ã€‚  
ç´§æŽ¥ç€ï¼Œä½œè€…æå‡ºäº†ä»–ä»¬çš„ç›®æ ‡ï¼š  

> The **goal** is for the agents to find cooperative policies that will maximize the *social welfare* which is defined as the sum of the rewards of each agent across the entire episode.  

ç®€è¨€ä¹‹ï¼Œä½œè€…æ•´ç¯‡æ–‡ç« çš„ç›®æ ‡å°†ä¼šæ˜¯å®žçŽ°ç¤¾ä¼šç¦åˆ©æœ€å¤§åŒ–ã€‚  

------

## 2 Model and Goal  
**Stochastic game**: Each agent has its own individual reward by modeling it with a stochastic game.  
Denotation:  
$n$: 	number of agents.  
$a\in A=\{1,2,...n\}$: 	index of one agent.  
$-a$: 	the set of other agents $A/a$.  
$U\_{a}$: 	set of actions of $a$.  
$o\_{a}$: 	agent a's partial observation at each time step, it is determined by the **observation function** $O(s,a)$ where $s\in S$ is the true state.  
$\pi\_{a}(u_{a}|o\_{a})$: 	policy, which is conditioned on its observation to decide its action $u\_{a}\in U\_{a}$.  
$\pi = [\pi\_{a}]_{a\in A} $ (è¡¨ç¤ºæ‰€æœ‰agentçš„policy).  
$O$: 	the set of observations from all agents at a time.  
$u\in U$:	 joint action of all the agents, where $U:=U_{1}\times \cdots \times U_{n}$ is the set of all joint actions. Each agent will take an action and these actions will form a joint action u, and then a transition occurs.  
$P(s^{'}|s,u)$: 	transition function.     $r\_{a}(s,u)$: reward.       $\gamma$: discount factor.   

**Semi-cooperative tasks**: the set of tasks where each agent may have a separate reward function but may benefit from cooperative strategies.  

**Goal: Maximizing the social welfare.**   
*social welfare*: the sum of the discounted rewards of all agents over the entire episode. The authors want to maximize it and obtain the socially optimal policy:
$$
\pi^{*}=\arg \max_{\pi}\sum_{a\in A}E_{s\sim\rho^{\pi},u\sim\pi}[r_{a}(s,u)]
$$
where  $E_{s\sim \rho^{\pi}}[Â·]$ denotes the expected reward with respect to discounted state distribution $\rho^{\pi}$ by initial state distribution and state transition dynamics distribution.  

## 3 Methods  

### 3.1 Change of Games via Reward Reshaping  
Each agent only has access to its partial observation and chooses its action based on its **local** best response. The authors will change the game by reshaping agentsâ€™ reward functions over time, so that the agentsâ€™ actions from the local best responses become socially optimal ones that maximize the cooperation effect. These reshaped rewards are called *well-coordinated rewards*.  
Then, run the following loop:  
(Denote by $G\_{t}$ and $\hat{\mathbf{r}}^{t}=(\hat{r}^{t}_{a}:a\in A)$  the game and its reward function at time step t = 0, 1, Â· Â· Â· .)  
```
(i) Compute the optimal policy ðœ‹ð‘¡ from ðºð‘¡
(ii) Evaluate how well-coordinated ðºð‘¡ is by evaluating ðœ‹ð‘¡
(iii) Update from ðºð‘¡ to ðºð‘¡+1 by updating from Ë†ð’“ð‘¡ to Ë†ð’“ð‘¡+1, using
the â€˜evaluation feedbackâ€™ from (ii)
(iv) Increment ð‘¡ and go to (i).
```
The converge time will be large so the authors take an approach of running the following two updates in parallel:  
```
Policy update: ð…ð‘¡+1 = ð¹ (ð…ð‘¡ , Ë†ð’“ð‘¡ )
Reward update: ð’“ð‘¡+1 = ð»( Ë†ð’“ð‘¡ , ð…ð‘¡ )
```
>In the policy update *F* , each agentâ€™s policy ð…ð‘¡ is updated to maximize the reshaped reward Ë†ð’“ð‘¡ at that time that can be done by a reinforcement learning algorithm, e.g., DQN. In the reward update ð», agents estimate the value of current policies and update the reshaped reward appropriately.  

å‡½æ•°Få¯¹åº”policy updateï¼Œé€šè¿‡æœ€å¤§åŒ–å½“å‰çš„reshaped reward ræ¥æ›´æ–°æ¯ä¸ªagentçš„ç­–ç•¥ï¼›å‡½æ•°Hå¯¹åº”reward updateï¼Œé€šè¿‡å¯¹å½“å‰ç­–ç•¥è¿›è¡Œä¼°å€¼æ¥è¿‘ä¼¼æ›´æ–°reshaped rewardã€‚  

### 3.2 Reward Update with Peer Evaluation  
each agent ð‘˜ generates a *counterfactual evaluation signal* (CES) $z^{t}\_{k}$ for the observed transition $o^{t}\_{k}->o^{t+1}_{k}$.  

